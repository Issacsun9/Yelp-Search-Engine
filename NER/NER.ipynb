{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48587c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.2.0/en_core_web_lg-3.2.0-py3-none-any.whl (777.4 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in f:\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (20.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: pathy>=0.3.5 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.19.2)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: click<8.1.0 in f:\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in f:\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in f:\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in f:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in f:\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (2.10)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in f:\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-lg==3.2.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.2.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81b9a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5ed95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_df = pd.read_csv(\"food.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03a0936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fdc_id</th>\n",
       "      <th>data_type</th>\n",
       "      <th>description</th>\n",
       "      <th>food_category_id</th>\n",
       "      <th>publication_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1105904</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>WESSON Vegetable Oil 1 GAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1105905</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>SWANSON BROTH BEEF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1105906</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1105907</td>\n",
       "      <td>branded_food</td>\n",
       "      <td>CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1105898</td>\n",
       "      <td>experimental_food</td>\n",
       "      <td>Discrepancy between the Atwater factor predict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-10-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fdc_id          data_type  \\\n",
       "0  1105904       branded_food   \n",
       "1  1105905       branded_food   \n",
       "2  1105906       branded_food   \n",
       "3  1105907       branded_food   \n",
       "4  1105898  experimental_food   \n",
       "\n",
       "                                         description  food_category_id  \\\n",
       "0                         WESSON Vegetable Oil 1 GAL               NaN   \n",
       "1                                 SWANSON BROTH BEEF               NaN   \n",
       "2           CAMPBELL'S SLOW KETTLE SOUP CLAM CHOWDER               NaN   \n",
       "3        CAMPBELL'S SLOW KETTLE SOUP CHEESE BROCCOLI               NaN   \n",
       "4  Discrepancy between the Atwater factor predict...               NaN   \n",
       "\n",
       "  publication_date  \n",
       "0       2020-11-13  \n",
       "1       2020-11-13  \n",
       "2       2020-11-13  \n",
       "3       2020-11-13  \n",
       "4       2020-10-30  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40764bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1605401"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_df[\"description\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341e1f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41363"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diaqualify foods with special characters, lowercase and extract results from \"description\" column\n",
    "foods = food_df[food_df[\"description\"].str.contains(\"[^a-zA-Z ]\") == False][\"description\"].apply(lambda food: food.lower())\n",
    "\n",
    "# filter out foods with more than 3 words, drop any duplicates\n",
    "foods = foods[foods.str.split().apply(len) <= 3].drop_duplicates()\n",
    "\n",
    "# print the remaining size\n",
    "foods.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a1c9961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find one-worded, two-worded and three-worded foods\n",
    "one_worded_foods = foods[foods.str.split().apply(len) == 1]\n",
    "two_worded_foods = foods[foods.str.split().apply(len) == 2]\n",
    "three_worded_foods = foods[foods.str.split().apply(len) == 3]\n",
    "\n",
    "# create a bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar([1, 2, 3], [one_worded_foods.size, two_worded_foods.size, three_worded_foods.size])\n",
    "\n",
    "# label the x-axis instances\n",
    "ax.set_xticks([1, 2, 3])\n",
    "ax.set_xticklabels([\"one\", \"two\", \"three\"])\n",
    "\n",
    "# set the title and the xy-axis labels\n",
    "plt.title(\"Number of Words in Food Entities\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Food Entities\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98767734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-worded food entities: 1443\n",
      "2-worded food entities: 962\n",
      "3-worded food entities: 802\n"
     ]
    }
   ],
   "source": [
    "# total number of foods\n",
    "total_num_foods = round(one_worded_foods.size / 45 * 100)\n",
    "\n",
    "# shuffle the 2-worded and 3-worded foods since we'll be slicing them\n",
    "two_worded_foods = two_worded_foods.sample(frac=1)\n",
    "three_worded_foods = three_worded_foods.sample(frac=1)\n",
    "\n",
    "# append the foods together \n",
    "foods = one_worded_foods.append(two_worded_foods[:round(total_num_foods * 0.30)]).append(three_worded_foods[:round(total_num_foods * 0.25)])\n",
    "\n",
    "# print the resulting sizes\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}-worded food entities:\", foods[foods.str.split().apply(len) == i + 1].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad53948",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_templates = [\n",
    "    \"I ate my {}\",\n",
    "    \"I'm eating a {}\",\n",
    "    \"I just ate a {}\",\n",
    "    \"I only ate the {}\",\n",
    "    \"I'm done eating a {}\",\n",
    "    \"I've already eaten a {}\",\n",
    "    \"I just finished my {}\",\n",
    "    \"When I was having lunch I ate a {}\",\n",
    "    \"I had a {} and a {} today\",\n",
    "    \"I ate a {} and a {} for lunch\",\n",
    "    \"I made a {} and {} for lunch\",\n",
    "    \"I ate {} and {}\",\n",
    "    \"today I ate a {} and a {} for lunch\",\n",
    "    \"I had {} with my husband last night\",\n",
    "    \"I brought you some {} on my birthday\",\n",
    "    \"I made {} for yesterday's dinner\",\n",
    "    \"last night, a {} was sent to me with {}\",\n",
    "    \"I had {} yesterday and I'd like to eat it anyway\",\n",
    "    \"I ate a couple of {} last night\",\n",
    "    \"I had some {} at dinner last night\",\n",
    "    \"Last night, I ordered some {}\",\n",
    "    \"I made a {} last night\",\n",
    "    \"I had a bowl of {} with {} and I wanted to go to the mall today\",\n",
    "    \"I brought a basket of {} for breakfast this morning\",\n",
    "    \"I had a bowl of {}\",\n",
    "    \"I ate a {} with {} in the morning\",\n",
    "    \"I made a bowl of {} for my breakfast\",\n",
    "    \"There's {} for breakfast in the bowl this morning\",\n",
    "    \"This morning, I made a bowl of {}\",\n",
    "    \"I decided to have some {} as a little bonus\",\n",
    "    \"I decided to enjoy some {}\",\n",
    "    \"I've decided to have some {} for dessert\",\n",
    "    \"I had a {}, a {} and {} at home\",\n",
    "    \"I took a {}, {} and {} on the weekend\",\n",
    "    \"I ate a {} with {} and {} just now\",\n",
    "    \"Last night, I ate an {} with {} and {}\",\n",
    "    \"I tasted some {}, {} and {} at the office\",\n",
    "    \"There's a basket of {}, {} and {} that I consumed\",\n",
    "    \"I devoured a {}, {} and {}\",\n",
    "    \"I've already had a bag of {}, {} and {} from the fridge\",\n",
    "    \"{} is delicious\",\n",
    "    \"{} is great\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "393911be",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}\n",
    "\n",
    "TEST_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}\n",
    "\n",
    "# one_food, two_food, and three_food combinations will be limited to 167 sentences\n",
    "FOOD_SENTENCE_LIMIT = 167\n",
    "\n",
    "# helper function for deciding what dictionary and subsequent array to append the food sentence on to\n",
    "def get_food_data(count):\n",
    "    return {\n",
    "        1: TRAIN_FOOD_DATA[\"one_food\"] if len(TRAIN_FOOD_DATA[\"one_food\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"one_food\"],\n",
    "        2: TRAIN_FOOD_DATA[\"two_foods\"] if len(TRAIN_FOOD_DATA[\"two_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"two_foods\"],\n",
    "        3: TRAIN_FOOD_DATA[\"three_foods\"] if len(TRAIN_FOOD_DATA[\"three_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"three_foods\"],\n",
    "    }[count]\n",
    "\n",
    "# the pattern to replace from the template sentences\n",
    "pattern_to_replace = \"{}\"\n",
    "\n",
    "# shuffle the data before starting\n",
    "foods = foods.sample(frac=1)\n",
    "\n",
    "# the count that helps us decide when to break from the for loop\n",
    "food_entity_count = foods.size - 1\n",
    "\n",
    "# start the while loop, ensure we don't get an index out of bounds error\n",
    "while food_entity_count >= 2:\n",
    "    entities = []\n",
    "\n",
    "    # pick a random food template\n",
    "    sentence = food_templates[random.randint(0, len(food_templates) - 1)]\n",
    "\n",
    "    # find out how many braces \"{}\" need to be replaced in the template\n",
    "    matches = re.findall(pattern_to_replace, sentence)\n",
    "\n",
    "    # for each brace, replace with a food entity from the shuffled food data\n",
    "    for match in matches:\n",
    "        food = foods.iloc[food_entity_count]\n",
    "        food_entity_count -= 1\n",
    "\n",
    "        # replace the pattern, but then find the match of the food entity we just inserted\n",
    "        sentence = sentence.replace(match, food, 1)\n",
    "        match_span = re.search(food, sentence).span()\n",
    "\n",
    "        # use that match to find the index positions of the food entity in the sentence, append\n",
    "        entities.append((match_span[0], match_span[1], \"FOOD\"))\n",
    "\n",
    "    # append the sentence and the position of the entities to the correct dictionary and array\n",
    "    get_food_data(len(matches)).append((sentence, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b73cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 one_food sentences: (\"I've already eaten a pastel pink sixlets\", {'entities': [(21, 40, 'FOOD')]})\n",
      "167 two_foods sentences: ('I ate a crisp granola and a mackerel fillets for lunch', {'entities': [(8, 21, 'FOOD'), (28, 44, 'FOOD')]})\n",
      "167 three_foods sentences: ('Last night, I ate an pasta trpina rigatoni with edam and chocolate eclair bar', {'entities': [(21, 42, 'FOOD'), (48, 52, 'FOOD'), (57, 77, 'FOOD')]})\n"
     ]
    }
   ],
   "source": [
    "for key in TRAIN_FOOD_DATA:\n",
    "    print(\"{} {} sentences: {}\".format(len(TRAIN_FOOD_DATA[key]), key, TRAIN_FOOD_DATA[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d09fe6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1117 one_food items: (\"I'm eating a carne guisada\", {'entities': [(13, 26, 'FOOD')]})\n",
      "213 two_foods items: ('I had a bowl of waffles  with mixed herbs and I wanted to go to the mall today', {'entities': [(16, 24, 'FOOD'), (30, 41, 'FOOD')]})\n",
      "220 three_foods items: ('I ate a flan with deli onion rye and pasta zero just now', {'entities': [(8, 12, 'FOOD'), (18, 32, 'FOOD'), (37, 47, 'FOOD')]})\n"
     ]
    }
   ],
   "source": [
    "for key in TEST_FOOD_DATA:\n",
    "    print(\"{} {} items: {}\".format(len(TEST_FOOD_DATA[key]), key, TEST_FOOD_DATA[key][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a57b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df_1 = pd.read_csv(\"learn-ai-bbc/BBC News Train.csv\")[[\"Text\"]]\n",
    "ner_df_2 = pd.read_csv(\"learn-ai-bbc/BBC News Test.csv\")[[\"Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4bdd588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  worldcom ex-boss launches defence lawyers defe...\n",
       "1  german business confidence slides german busin...\n",
       "2  bbc poll indicates economic gloom citizens in ...\n",
       "3  lifestyle  governs mobile choice  faster  bett...\n",
       "4  enron bosses in $168m payout eighteen former e..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df = pd.concat([ner_df_1, ner_df_2])\n",
    "ner_df.columns = [[\"Article\"]]\n",
    "ner_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7172715c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  worldcom ex-boss launches defence lawyers defe...\n",
       "1  german business confidence slides german busin...\n",
       "2  bbc poll indicates economic gloom citizens in ...\n",
       "3  lifestyle  governs mobile choice  faster  bett...\n",
       "4  enron bosses in $168m payout eighteen former e..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npr_df = pd.DataFrame(ner_df.apply(lambda x: x.values[0], axis=1), columns=['Article'])\n",
    "npr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81001f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    '''\n",
    "    Preprocess a string.\n",
    "    :parameter\n",
    "        :param text: string - name of column containing text\n",
    "        :param lst_stopwords: list - list of stopwords to remove\n",
    "        :param flg_stemm: bool - whether stemming is to be applied\n",
    "        :param flg_lemm: bool - whether lemmitisation is to be applied\n",
    "    :return\n",
    "        cleaned text\n",
    "    '''\n",
    "    ## clean (convert to lowercase and remove punctuations and   \n",
    "    # characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c38ef017",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb66bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "revision_texts = []\n",
    "\n",
    "# convert the articles to spacy objects to better identify the sentences. Disabled unneeded components. # takes ~ 4 minutes\n",
    "for doc in nlp.pipe(npr_df[\"Article\"], batch_size=30, disable=[\"tagger\", \"ner\"]):\n",
    "    for sentence in doc.sents:\n",
    "        if  40 < len(sentence.text) < 200:\n",
    "            # some of the sentences had excessive whitespace in between words, so we're trimming that\n",
    "            revision_texts.append(\" \".join(re.split(\"\\s+\", sentence.text, flags=re.UNICODE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5c3f95ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37143"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(revision_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fa34c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "revisions = []\n",
    "\n",
    "# Use the existing spaCy model to predict the entities, then append them to revision\n",
    "for doc in nlp.pipe(revision_texts, batch_size=50, disable=[\"tagger\", \"parser\"]):\n",
    "    \n",
    "    # don't append sentences that have no entities\n",
    "    if len(doc.ents) > 0:\n",
    "        revisions.append((doc.text, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in doc.ents]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0e4066ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.\n",
      "{'entities': [(0, 8, 'ORG'), (59, 67, 'ORG'), (74, 80, 'PERSON'), (168, 173, 'ORDINAL')]}\n"
     ]
    }
   ],
   "source": [
    "# print an example of the revision sentence\n",
    "print(revisions[0][0])\n",
    "\n",
    "# print an example of the revision data\n",
    "print(revisions[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e9dbac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays to store the revision data\n",
    "TRAIN_REVISION_DATA = []\n",
    "TEST_REVISION_DATA = []\n",
    "\n",
    "# create dictionaries to keep count of the different entities\n",
    "TRAIN_ENTITY_COUNTER = {}\n",
    "TEST_ENTITY_COUNTER = {}\n",
    "\n",
    "# This will help distribute the entities (i.e. we don't want 1000 PERSON entities, but only 80 ORG entities)\n",
    "REVISION_SENTENCE_SOFT_LIMIT = 100\n",
    "\n",
    "# helper function for incrementing the revision counters\n",
    "def increment_revision_counters(entity_counter, entities):\n",
    "    for entity in entities:\n",
    "        label = entity[2]\n",
    "        if label in entity_counter:\n",
    "            entity_counter[label] += 1\n",
    "        else:\n",
    "            entity_counter[label] = 1\n",
    "\n",
    "random.shuffle(revisions)\n",
    "for revision in revisions:\n",
    "    # get the entities from the revision sentence\n",
    "    entities = revision[1][\"entities\"]\n",
    "\n",
    "    # simple hack to make sure spaCy entities don't get too one-sided\n",
    "    should_append_to_train_counter = 0\n",
    "    for _, _, label in entities:\n",
    "        if label in TRAIN_ENTITY_COUNTER and TRAIN_ENTITY_COUNTER[label] > REVISION_SENTENCE_SOFT_LIMIT:\n",
    "            should_append_to_train_counter -= 1\n",
    "        else:\n",
    "            should_append_to_train_counter += 1\n",
    "\n",
    "    # simple switch for deciding whether to append to train data or test data\n",
    "    if should_append_to_train_counter >= 0:\n",
    "        TRAIN_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TRAIN_ENTITY_COUNTER, entities)\n",
    "    else:\n",
    "        TEST_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TEST_ENTITY_COUNTER, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "feebf5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOOD 501\n",
      "REVISION 1018\n",
      "COMBINED 1519\n"
     ]
    }
   ],
   "source": [
    "# combine the food training data\n",
    "TRAIN_FOOD_DATA_COMBINED = TRAIN_FOOD_DATA[\"one_food\"] + TRAIN_FOOD_DATA[\"two_foods\"] + TRAIN_FOOD_DATA[\"three_foods\"]\n",
    "\n",
    "# print the length of the food training data\n",
    "print(\"FOOD\", len(TRAIN_FOOD_DATA_COMBINED))\n",
    "\n",
    "# print the length of the revision training data\n",
    "print(\"REVISION\", len(TRAIN_REVISION_DATA))\n",
    "\n",
    "# join and print the combined length\n",
    "TRAIN_DATA = TRAIN_REVISION_DATA + TRAIN_FOOD_DATA_COMBINED\n",
    "print(\"COMBINED\", len(TRAIN_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f9dc426e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I ate a sour gummi santas with applesauce  and ita...\" with entities \"[(8, 25, 'FOOD'), (31, 42, 'FOOD'), (47, 66, 'FOOD...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I've decided to have some stewed tomatoes  for des...\" with entities \"[(26, 42, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I had a apples , a potato sticks and stuffed sandw...\" with entities \"[(8, 15, 'FOOD'), (19, 32, 'FOOD'), (37, 55, 'FOOD...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I had a bowl of peanuts \" with entities \"[(16, 24, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I brought a basket of original instant oatmeal  fo...\" with entities \"[(22, 47, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I tasted some natural cheddar, toastables and cook...\" with entities \"[(14, 29, 'FOOD'), (31, 41, 'FOOD'), (46, 54, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"today I ate a flapjack and a lemonade  for lunch\" with entities \"[(14, 22, 'FOOD'), (29, 38, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I tasted some hot dark chocolate, organic sesame o...\" with entities \"[(14, 32, 'FOOD'), (34, 52, 'FOOD'), (57, 68, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I took a nougat, tilapia fillets and maraschino  o...\" with entities \"[(9, 15, 'FOOD'), (17, 32, 'FOOD'), (37, 48, 'FOOD...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I had a quencher, a couscous  and lime beans at ho...\" with entities \"[(8, 16, 'FOOD'), (20, 29, 'FOOD'), (34, 44, 'FOOD...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "F:\\anaconda3\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"There's a basket of kosher spears , maltose and ly...\" with entities \"[(20, 34, 'FOOD'), (36, 43, 'FOOD'), (48, 55, 'FOO...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses (1/30) {'ner': 2696.560790725156}\n",
      "Losses (2/30) {'ner': 1700.5449324950175}\n",
      "Losses (3/30) {'ner': 1376.1217372468611}\n",
      "Losses (4/30) {'ner': 1206.2528038390888}\n",
      "Losses (5/30) {'ner': 1133.768218684352}\n",
      "Losses (6/30) {'ner': 941.3115645749045}\n",
      "Losses (7/30) {'ner': 957.0509242290414}\n",
      "Losses (8/30) {'ner': 833.8071583984771}\n",
      "Losses (9/30) {'ner': 820.4601613202711}\n",
      "Losses (10/30) {'ner': 815.6817262677974}\n",
      "Losses (11/30) {'ner': 736.282218189483}\n",
      "Losses (12/30) {'ner': 701.9625430330875}\n",
      "Losses (13/30) {'ner': 717.2771084666373}\n",
      "Losses (14/30) {'ner': 616.3865549564459}\n",
      "Losses (15/30) {'ner': 628.703534421355}\n",
      "Losses (16/30) {'ner': 554.7165739578395}\n",
      "Losses (17/30) {'ner': 635.7095733933234}\n",
      "Losses (18/30) {'ner': 516.003822191571}\n",
      "Losses (19/30) {'ner': 507.82512237553743}\n",
      "Losses (20/30) {'ner': 554.1818542134298}\n",
      "Losses (21/30) {'ner': 456.1951370228754}\n",
      "Losses (22/30) {'ner': 469.80061714713736}\n",
      "Losses (23/30) {'ner': 401.00576111448726}\n",
      "Losses (24/30) {'ner': 397.909995311683}\n",
      "Losses (25/30) {'ner': 375.6947720676737}\n",
      "Losses (26/30) {'ner': 433.1803328588944}\n",
      "Losses (27/30) {'ner': 489.7759106862705}\n",
      "Losses (28/30) {'ner': 433.43375620592786}\n",
      "Losses (29/30) {'ner': 370.8611532546919}\n",
      "Losses (30/30) {'ner': 428.5271825212665}\n"
     ]
    }
   ],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"FOOD\")\n",
    "from spacy.training import Example\n",
    "\n",
    "# get the names of the components we want to disable during training\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# start the training loop, only training NER\n",
    "epochs = 30\n",
    "optimizer = nlp.resume_training()\n",
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    \n",
    "    # batch up the examples using spaCy's minibatc\n",
    "    for epoch in range(epochs):\n",
    "        examples = TRAIN_DATA\n",
    "        random.shuffle(examples)\n",
    "        batches = minibatch(examples, size=sizes)\n",
    "        losses = {}\n",
    "        \n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example = []\n",
    "        # Update the model with iterating each text\n",
    "            for i in range(len(texts)):\n",
    "                doc = nlp.make_doc(texts[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "        \n",
    "            nlp.update(example, sgd=optimizer, drop=0.35, losses=losses)\n",
    "\n",
    "        print(\"Losses ({}/{})\".format(epoch + 1, epochs), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "158444c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I had a \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hamburger\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chips\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " for lunch today.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I decided to have \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chocolate ice cream\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " as a little treat for myself.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I ordered \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    basmati rice\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    leaf spinach\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    cheese\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FOOD</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " yesterday</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spacy.displacy.render(nlp(\"I had a hamburger and chips for lunch today.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I decided to have chocolate ice cream as a little treat for myself.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I ordered basmati rice, leaf spinach and cheese from Tesco yesterday\"), style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "28f8be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to hold our evaluation data\n",
    "food_evaluation = {\n",
    "    \"one_food\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "    },\n",
    "    \"two_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"three_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "word_evaluation = {\n",
    "    \"1_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"2_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"3_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# loop over data from our test food set (3 keys in total)\n",
    "for key in TEST_FOOD_DATA:\n",
    "    foods = TEST_FOOD_DATA[key]\n",
    "\n",
    "    for food in foods:\n",
    "        # extract the sentence and correct food entities according to our test data\n",
    "        sentence = food[0]\n",
    "        entities = food[1][\"entities\"]\n",
    "\n",
    "        # for each entity, use our updated model to make a prediction on the sentence\n",
    "        for entity in entities:\n",
    "            doc = nlp(sentence)\n",
    "            correct_text = sentence[entity[0]:entity[1]]\n",
    "            n_worded_food =  len(correct_text.split())\n",
    "\n",
    "            # if we find that there's a match for predicted entity and predicted text, increment correct counters\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                    food_evaluation[key][\"correct\"] += 1\n",
    "                    if n_worded_food > 0:\n",
    "                        word_evaluation[f\"{n_worded_food}_worded_foods\"][\"correct\"] += 1\n",
    "                    \n",
    "                    # this break is important, ensures that we're not double counting on a correct match\n",
    "                    break\n",
    "            \n",
    "            #  increment total counters after each entity loop\n",
    "            food_evaluation[key][\"total\"] += 1\n",
    "            if n_worded_food > 0:\n",
    "                word_evaluation[f\"{n_worded_food}_worded_foods\"][\"total\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1d35f0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_worded_foods: 95.02%\n",
      "2_worded_foods: 96.51%\n",
      "3_worded_foods: 96.01%\n",
      "---\n",
      "one_food: 95.17%\n",
      "two_foods: 94.84%\n",
      "three_foods: 97.27%\n",
      "\n",
      "Total: 95.73%\n"
     ]
    }
   ],
   "source": [
    "for key in word_evaluation:\n",
    "    correct = word_evaluation[key][\"correct\"]\n",
    "    total = word_evaluation[key][\"total\"]\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "food_total_sum = 0\n",
    "food_correct_sum = 0\n",
    "\n",
    "print(\"---\")\n",
    "for key in food_evaluation:\n",
    "    correct = food_evaluation[key][\"correct\"]\n",
    "    total = food_evaluation[key][\"total\"]\n",
    "    \n",
    "    food_total_sum += total\n",
    "    food_correct_sum += correct\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal: {food_correct_sum/food_total_sum * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d7517358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary which will be populated with the entities and result information\n",
    "entity_evaluation = {}\n",
    "\n",
    "# helper function to udpate the entity_evaluation dictionary\n",
    "def update_results(entity, metric):\n",
    "    if entity not in entity_evaluation:\n",
    "        entity_evaluation[entity] = {\"correct\": 0, \"total\": 0}\n",
    "    \n",
    "    entity_evaluation[entity][metric] += 1\n",
    "\n",
    "# same as before, see if entities from test set match what spaCy currently predicts\n",
    "for data in TEST_REVISION_DATA:\n",
    "    sentence = data[0]\n",
    "    entities = data[1][\"entities\"]\n",
    "\n",
    "    for entity in entities:\n",
    "        doc = nlp(sentence)\n",
    "        correct_text = sentence[entity[0]:entity[1]]\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                update_results(ent.label_, \"correct\")\n",
    "                break\n",
    "\n",
    "        update_results(entity[2], \"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fba9d70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE | 83.22%\n",
      "ORG | 46.17%\n",
      "GPE | 75.52%\n",
      "PERSON | 66.10%\n",
      "ORDINAL | 90.80%\n",
      "CARDINAL | 65.76%\n",
      "NORP | 92.39%\n",
      "PERCENT | 94.33%\n",
      "EVENT | 73.73%\n",
      "MONEY | 86.57%\n",
      "QUANTITY | 77.38%\n",
      "PRODUCT | 54.17%\n",
      "TIME | 77.73%\n",
      "FAC | 60.42%\n",
      "LOC | 82.69%\n",
      "WORK_OF_ART | 33.33%\n",
      "LAW | 65.52%\n",
      "LANGUAGE | 81.25%\n",
      "\n",
      "Overall accuracy: 72.71%\n"
     ]
    }
   ],
   "source": [
    "sum_total = 0\n",
    "sum_correct = 0\n",
    "\n",
    "for entity in entity_evaluation:\n",
    "    total = entity_evaluation[entity][\"total\"]\n",
    "    correct = entity_evaluation[entity][\"correct\"]\n",
    "\n",
    "    sum_total += total\n",
    "    sum_correct += correct\n",
    "    \n",
    "    print(\"{} | {:.2f}%\".format(entity, correct / total * 100))\n",
    "\n",
    "print()\n",
    "print(\"Overall accuracy: {:.2f}%\".format(sum_correct / sum_total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "286dd075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_trained_model.pkl']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(nlp, 'my_trained_model.pkl', compress=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9b699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
