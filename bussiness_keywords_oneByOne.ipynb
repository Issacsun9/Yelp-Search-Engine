{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WeptlGXN2MnF",
    "outputId": "106a18a1-4d8f-4156-a45a-bb24c1a0a092"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HUy0jqfyrCDE"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(sen, stop_words):\n",
    "  # USE: function to remove stopwords\n",
    "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
    "  return sen_new\n",
    "\n",
    "def summarize(df, len_summary, show_summary = False):\n",
    "  # USE: Summarize the sentences in the reviews for a bussiness\n",
    "  # INPUT: \n",
    "    # df: each row of which is a review for this bussiness, df\n",
    "    # len_summary: number of sentences in summary for output, int\n",
    "    # show_summary: print summary or not, boolean\n",
    "  # OUTPUT: sorted sentences in all reviews, list of tuples\n",
    "\n",
    "  # split the the text in the articles into sentences\n",
    "  sentences = []\n",
    "  for s in df['text']:\n",
    "    sentences.append(sent_tokenize(s))  \n",
    "  sentences = [y for x in sentences for y in x]\n",
    "  # remove punctuations, numbers and special characters\n",
    "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "  # make alphabets lowercase\n",
    "  clean_sentences = [s.lower() for s in clean_sentences]\n",
    "  # remove stopwords from the sentences\n",
    "  stop_words = stopwords.words('english')\n",
    "  clean_sentences = [remove_stopwords(r.split(), stop_words) for r in clean_sentences]\n",
    "\n",
    "  # Extract word vectors\n",
    "  word_embeddings = {}\n",
    "  f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "  for line in f:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      word_embeddings[word] = coefs\n",
    "  f.close()  \n",
    "  \n",
    "  # Extract sentence vectors\n",
    "  sentence_vectors = []\n",
    "  for i in clean_sentences:\n",
    "    if len(i) != 0:\n",
    "      v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "    else:\n",
    "      v = np.zeros((100,))\n",
    "    sentence_vectors.append(v)\n",
    "\n",
    "  # Similarities among the sentences\n",
    "  sim_mat = np.zeros([len(sentences), len(sentences)])\n",
    "  for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "      if i != j:\n",
    "        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n",
    "  nx_graph = nx.from_numpy_array(sim_mat)\n",
    "  scores = nx.pagerank(nx_graph)\n",
    "  ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "  \n",
    "  # Generate summary\n",
    "  if show_summary == True:\n",
    "    for i in range(len_summary):\n",
    "      print(ranked_sentences[i][1])\n",
    "\n",
    "  return ranked_sentences[0 : len_summary]\n",
    "\n",
    "def get_keywords(text, numOfKeywords = 10, max_ngram_size = 2, deduplication_threshold = 0.7, windowsSize = 5, show_keywords = False):\n",
    "  # USE: get the keywords in a paragraph, str\n",
    "    if type(text) is list:\n",
    "      text = str(text).replace(\"', '\", ' ').replace(\"['\",'').replace(\"']\",\"\")\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan='en', n=max_ngram_size, dedupLim=deduplication_threshold, dedupFunc='seqm', top=numOfKeywords, features=None)\n",
    "    keywords_all = custom_kw_extractor.extract_keywords(text)\n",
    "    keywords = [item[0] for item in keywords_all]\n",
    "    if show_keywords == True:\n",
    "      for keyword in keywords:\n",
    "        print(keyword)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food bad',\n",
       " 'chinese restaurant',\n",
       " 'spaghetti meatball',\n",
       " 'meatball menu',\n",
       " 'bad restaurant',\n",
       " 'restaurant feel',\n",
       " 'chinese',\n",
       " 'food',\n",
       " 'nice',\n",
       " 'bad']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If work on str input\n",
    "testText = \"food bad restaurant feel like old chinese restaurant got confused saw spaghetti meatball menu since chinese restaurant decided try well nice\"\n",
    "get_keywords(testText)\n",
    "\n",
    "# If work on list of str input\n",
    "testText = ['food', 'bad', 'restaurant', 'feel', 'like', 'old', 'chinese', 'restaurant', 'got', 'confused', 'saw', 'spaghetti', 'meatball', 'menu', 'since', 'chinese', 'restaurant', 'decided', 'try', 'well', 'nice']\n",
    "get_keywords(testText)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Summarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cse6242]",
   "language": "python",
   "name": "conda-env-cse6242-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
